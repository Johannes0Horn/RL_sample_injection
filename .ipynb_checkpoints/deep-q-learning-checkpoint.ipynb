{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$ networks (DQN)\n",
    "\n",
    "In previous notebooks, we have seen how we can use `tensorflow` and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\\mathcal{S}$ and action spaces $\\mathcal{A}$, we run into problems when our observation space is continuous (or even just large!).\n",
    "\n",
    "Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge.\n",
    "\n",
    "Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a **differentiable function approximator**, we get this kind of behaviour. Recall that $Q$-learning is a *regression* problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by *deep* reinforcement learning researchers is the *deep* neural network.\n",
    "\n",
    "If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. ([Pieter Abbeel](https://www.youtube.com/watch?v=l-mYLq6eZPY) notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success).\n",
    "\n",
    "When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients.\n",
    "\n",
    "![**missing image (q-network)**](images/q-network.png)\n",
    "\n",
    "In this notebook, we make use of `tensorflow`'s `keras` API to build neural networks. We also take advantage of **batched environments** to accelerate data collection. The `keras` api build neural networks that process inputs in **batches**. This means that if our observation space for a single environment has a shape $84 \\times 84 \\times 3$, then the network expects inputs of shape $B \\times 84 \\times 84 \\times 3$ where $B$ is the number of inputs in the batch.\n",
    "\n",
    "When running the tabular $Q$-learning agent in `tensorflow` in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use `keras`, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can also stabilize training by diversifying the collected data over time.\n",
    "\n",
    "### Target Networks and Experience Replay\n",
    "\n",
    "In this notebook, we are going to implement two techniques required for stabilizing training: **target networks** and **experience replay**.\n",
    "\n",
    "### Target Networks\n",
    "\n",
    "Neural networks are *differentiable*, meaning that similar inputs produce similar outputs. In most environments, consecutive states are often similar to each other (with small changes occurring as a result of actions chosen). In deep $Q$-learning, we are doing a regression problem with a moving target - we are trying to predict our own output. When we do this, we perform a maximization step over our output:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q_\\theta(s_{t+1}, a_{t+1}) \\right) - Q_\\theta(s_t, a_t) \\right)^2\n",
    "$$\n",
    "\n",
    "Minimizing our prediction error, in general, will tend to increase the value of our prediction for $Q(s_t)$ because of this maximization step. This poses a problem. Consider the following sequence of events:\n",
    "1. The agent is in a state $s_t$\n",
    "2. The agent chooses an action that maximizes $Q(s_t)$\n",
    "3. The state transitions from $s_t$ to $s_{t+1}$, giving a reward of $r_t$ and a terminal flag $d_t$.\n",
    "4. Using the transition $s_t, a_t, r_t, s_{t+1}, d_t$, the agent minimizes $L(\\theta)$\n",
    "\n",
    "Since $s_{t+1}$ and $s_t$ are temporally close, they tend to be similar in general. During step 4, we updated our prediction for $Q(s_t)$ according to a maximization over all next possible values of $Q(s_{t+1})$. This tends to increase the value for $Q(s_t)$. When the agent goes to repeat this cycle the next time, its predictions for $Q(s_{t+1})$ will already be higher, because the values for $Q(s_t)$ are higher and $s_t \\approx s_{t+1}$.\n",
    "\n",
    "To handle this, we introduce a **target network** with parameters $\\theta^-$ which is used when computing the TD-target\n",
    "\n",
    "$$\n",
    "r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q_{\\theta^-}(s_{t+1}, a_{t+1}) \\right)\n",
    "$$\n",
    "\n",
    "This way, the act of updating $\\theta$ to minimize $L(\\theta)$ has no effect on our regression targets. To keep the predictions made by the target networks $Q_{\\theta^-}$ somewhat in line with the actual $Q$-network, we synchronize parameters every fixed number of timesteps.\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "The vanilla $Q$-learning algorithm (and DQN, as thus far described) uses only the most recent transition to train on. While this makes the agent good at predicting recent $Q$-values, it can cause it to perform worse on older or uncommon transitions. To stabilize training, we instead store transitions in memory and sample them randomly from batches to ensure the agent gets a good mix of experiences at each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1000000):\n",
    "        self.memory = deque(maxlen=size)\n",
    "        \n",
    "    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n",
    "        \n",
    "    def sample(self, num=32):\n",
    "        num = min(num, len(self.memory))\n",
    "        return random.sample(self.memory, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []):\n",
    "        self.epsilon_i = epsilon_i\n",
    "        self.epsilon_f = epsilon_f\n",
    "        self.n_epsilon = n_epsilon\n",
    "        self.epsilon = epsilon_i\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "        self.Q = tf.keras.models.Sequential()\n",
    "        self.Q.add(tf.keras.layers.Input(shape=state_shape))\n",
    "        for size in hidden_sizes:\n",
    "            self.Q.add(tf.keras.layers.Dense(size, activation='relu', use_bias='false', kernel_initializer='he_uniform', dtype='float64'))\n",
    "        self.Q.add(tf.keras.layers.Dense(self.num_actions, activation=\"linear\", use_bias='false', kernel_initializer='zeros', dtype='float64'))\n",
    "        \n",
    "        # target network\n",
    "        self.Q_ = tf.keras.models.Sequential()\n",
    "        self.Q_.add(tf.keras.layers.Input(shape=state_shape))\n",
    "        for size in hidden_sizes:\n",
    "            self.Q_.add(tf.keras.layers.Dense(size, activation='relu', use_bias='false', kernel_initializer='he_uniform', dtype='float64'))\n",
    "        self.Q_.add(tf.keras.layers.Dense(self.num_actions, activation=\"linear\", use_bias='false', kernel_initializer='zeros', dtype='float64'))\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(alpha)  \n",
    "    \n",
    "    def synchronize(self):\n",
    "        self.Q_.set_weights(self.Q.get_weights())\n",
    "\n",
    "    def act(self, s_t):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions, size=self.num_envs)\n",
    "        return np.argmax(self.Q(s_t), axis=1)\n",
    "    \n",
    "    def decay_epsilon(self, n):\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_f, \n",
    "            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n",
    "\n",
    "    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1)) # note we use Q_ \n",
    "            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1)\n",
    "            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)\n",
    "        grads = tape.gradient(loss, self.Q.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteToBoxWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete), \\\n",
    "            \"Should only be used to wrap Discrete envs.\"\n",
    "        self.n = self.observation_space.n\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (self.n,))\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        new_obs = np.zeros(self.n)\n",
    "        new_obs[obs] = 1\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for env_index in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, window=100):\n",
    "    sns.lineplot(\n",
    "        data=data.rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, T=20000, num_envs=32, batch_size=32, sync_every=100, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95):\n",
    "    env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs)\n",
    "    state_shape = env.observation_space.shape\n",
    "    num_actions = env.action_space.n\n",
    "    agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma)\n",
    "    rewards = []\n",
    "    buffer = ReplayBuffer()\n",
    "    episode_rewards = 0\n",
    "    s_t = env.reset()\n",
    "    for t in range(T):\n",
    "        if t%sync_every == 0:\n",
    "            agent.synchronize()\n",
    "        \n",
    "        a_t = agent.act(s_t)\n",
    "        s_t_next, r_t, d_t, info = env.step(a_t)\n",
    "        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)\n",
    "        s_t = s_t_next\n",
    "        for batch in buffer.sample(batch_size):\n",
    "            agent.update(*batch)\n",
    "        agent.decay_epsilon(t/T)\n",
    "        episode_rewards += r_t\n",
    "\n",
    "        for i in range(env.num_envs):\n",
    "            if d_t[i]:\n",
    "                rewards.append(episode_rewards[i])\n",
    "                episode_rewards[i] = 0\n",
    "                s_t[i] = env.reset_at(i)\n",
    "            \n",
    "    plot(pd.DataFrame(rewards), window=10)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "train(\"CartPole-v0\", T=20000, num_envs=32, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
